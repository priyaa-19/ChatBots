{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Checking for the API request\n",
        "\n"
      ],
      "metadata": {
        "id": "BXjk821PbPqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp0e3J6GbhIB",
        "outputId": "686c5a8f-0c46-482f-ba35-043834944593",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.58.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "ChatCompletion(id='chatcmpl-AkOhX9W9AX04xSoDUiZjUbkxEPoBt', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Once upon a time, in a distant future, there was a robot named Nova. Nova was an advanced artificial intelligence designed to perform tasks efficiently and without any form of emotion. However, Nova's creators decided to experiment and program Nova to learn and experience emotions.\\n\\nAt first, Nova was confused. Emotions were a foreign concept to her, and she struggled to understand them. She would observe humans interacting and expressing their emotions, trying to mimic their behavior. But she couldn't quite grasp the depth and\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735621403, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=17, total_tokens=117, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n",
        "import openai\n",
        "\n",
        "# Set your API key\n",
        "openai.api_key = \"api key\"\n",
        "# Make a request to the OpenAI model\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Specify the model you want to use\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Write a short story about a robot learning emotions.\"}], # Pass the prompt as a message\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hubR8eYWh90"
      },
      "source": [
        "# **Basic chatbot that will retreive data from only one document**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EROrgTgDWiWr",
        "outputId": "925225ef-737a-4d7a-bd9b-40737d59b24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.15-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.15 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.17-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.15->llama-index) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.11-py3-none-any.whl.metadata (912 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.15->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.15->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.15->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.15->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.15->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.15->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading llama_index-0.12.15-py3-none-any.whl (6.9 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.3-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.15-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.17-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.4-py3-none-any.whl (39 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.11-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
            "Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, PyPDF2, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed PyPDF2-3.0.1 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.11 llama-index-0.12.15 llama-index-agent-openai-0.4.3 llama-index-cli-0.4.0 llama-index-core-0.12.15 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.17 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.4 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pypdf-5.2.0 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index openai PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgsWPWDWSX3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    GPTVectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"API KEY here\"\n",
        "\n",
        "\n",
        "# Define paths\n",
        "pdf_directory = \"/content/data\"  # Directory containing your PDF file #create a folder and put all the data file there and give the path\n",
        "index_storage_directory = \"/content/storage\"  # Directory to store the index\n",
        "\n",
        "\n",
        "def create_index():\n",
        "\n",
        "    # Ensure the PDF directory exists and contains files\n",
        "    if not os.path.exists(pdf_directory) or not os.listdir(pdf_directory):\n",
        "        raise FileNotFoundError(f\"No PDF files found in directory: {pdf_directory}\")\n",
        "\n",
        "    # Step 1: Load data from the PDF\n",
        "    print(\"Loading data from PDF...\")\n",
        "    documents = SimpleDirectoryReader(pdf_directory).load_data()\n",
        "\n",
        "    # Step 2: Create the vector index\n",
        "    print(\"Creating the index...\")\n",
        "    index = GPTVectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    # Step 3: Persist the index\n",
        "    print(\"Saving the index...\")\n",
        "    index.storage_context.persist(index_storage_directory)\n",
        "\n",
        "    print(\"Index created and saved successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_index():\n",
        "    \"\"\"\n",
        "    Load an existing index from storage.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(index_storage_directory):\n",
        "        raise FileNotFoundError(f\"Index storage directory not found: {index_storage_directory}\")\n",
        "\n",
        "    print(\"Loading the index from storage...\")\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=index_storage_directory)\n",
        "\n",
        "    # Specify the index type when loading\n",
        "    index = load_index_from_storage(storage_context, index_cls=GPTVectorStoreIndex)\n",
        "    print(\"Index loaded successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def query_index(index, query):\n",
        "    \"\"\"\n",
        "    Query the vector index for an answer to the user's question.\n",
        "    \"\"\"\n",
        "    print(f\"Querying the index: {query}\")\n",
        "\n",
        "    # Use as_query_engine to perform the query\n",
        "    query_engine = index.as_query_engine()\n",
        "    response = query_engine.query(query)\n",
        "\n",
        "    return response.response\n",
        "\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Attempt to load the existing index\n",
        "        index = load_index()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"{e}\\nCreating a new index...\")\n",
        "        index = create_index()\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    # Chat loop for querying\n",
        "    print(\"Chatbot is ready! Ask questions about the PDF.\")\n",
        "    while True:\n",
        "        user_input = input(\"Your question (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Query the index and provide a response\n",
        "            answer = query_index(index, user_input)\n",
        "            print(f\"Answer: {answer}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during querying: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yYfvA_hd3Kw"
      },
      "source": [
        "# **Basic chatbot that will retreive data from all documents in the folder**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXoV_opLExjj",
        "outputId": "532e0af5-82c7-43fa-fd37-66ca228b2ded",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the PDF directory path (leave blank for default): /content/dataaaa\n",
            "Enter the index storage path (leave blank for default): /content/storage\n",
            "Index storage directory not found: /content/storage\n",
            "Creating a new index...\n",
            "Loading data from PDFs...\n",
            "Creating the index...\n",
            "Saving the index...\n",
            "Index created and saved successfully.\n",
            "Chatbot is ready! Ask questions about the PDFs.\n",
            "Your question (or type 'exit' to quit): List all the services provided by commedia\n",
            "Querying the index: List all the services provided by commedia\n",
            "Answer: Commedia provided professional services for delivering live content from stadiums to media houses, developed products for stadium connectivity, offered quality and experience monitoring services, provided integration and professional services for connectivity among Stadium, NOC, and studios, maintained workflow for delivery of QoS in studios, and managed the project within the stipulated timeframe in all locations.\n",
            "Your question (or type 'exit' to quit): Who is the COO of commedia\n",
            "Querying the index: Who is the COO of commedia\n",
            "Answer: Harshad Awasare\n",
            "Your question (or type 'exit' to quit): Who is the MD of commedia\n",
            "Querying the index: Who is the MD of commedia\n",
            "Answer: The Managing Director of Commedia is not explicitly mentioned in the provided context information.\n",
            "Your question (or type 'exit' to quit): All the projects undertaken by commedia in the year 2023\n",
            "Querying the index: All the projects undertaken by commedia in the year 2023\n",
            "Answer: Commedia's projects in the year 2023 are not mentioned in the provided context information.\n",
            "Your question (or type 'exit' to quit): does commedia provide solutions for  Wireline\n",
            "Querying the index: does commedia provide solutions for  Wireline\n",
            "Answer: Yes, Commedia provides solutions for Wireline networks.\n",
            "Your question (or type 'exit' to quit): Give some detailed overview of the projects done by the copany in wireline domai\n",
            "Querying the index: Give some detailed overview of the projects done by the copany in wireline domai\n",
            "Answer: The company has successfully completed projects in the wireline domain that involved designing, integrating, and implementing WAN network installations for a major GCC country's embassy missions worldwide. The scope of work included site surveys, installations, line up, commissioning, and setting up IP infrastructure across 108 sites. The technologies involved OEM such as Cisco, including ASR Routers and Nexus switches. Additionally, the company provided technical support and maintenance for four years. The projects were executed globally with installations and fieldwork progressing as planned, managed by Commedia. The benefits included having a trained and skilled support team available, a network of highly trained and dedicated professionals for timely project implementation, and ensuring no business impact for the end customer with timely global deployment.\n",
            "Your question (or type 'exit' to quit): Give information about all the management in the team\n",
            "Querying the index: Give information about all the management in the team\n",
            "Answer: The management team has a unique combination of business expertise, experience with startups, and knowledge of the industry. They employ flexible project execution methodologies with a focus on building strong client relationships. Additionally, they possess a strong understanding of the telecom industry and software technologies. The team has proven methodologies for project planning and execution for turnkey projects, along with a track record of providing quality managed services.\n",
            "Your question (or type 'exit' to quit): exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    GPTVectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "import openai\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Set your OpenAI API key securely\n",
        "openai.api_key = \"API KEY here\"\n",
        "\n",
        "# Define default paths\n",
        "pdf_directory = \"/content/dataaaa\"  # Directory containing your PDF files\n",
        "index_storage_directory = \"/content/storage\"  # Directory to store the index\n",
        "\n",
        "\n",
        "def create_index(pdf_dir, storage_dir):\n",
        "    \"\"\"\n",
        "    Creates a new index from PDFs in the given directory.\n",
        "    \"\"\"\n",
        "    # Ensure the PDF directory exists and contains files\n",
        "    if not os.path.exists(pdf_dir) or not os.listdir(pdf_dir):\n",
        "        raise FileNotFoundError(f\"No PDF files found in directory: {pdf_dir}\")\n",
        "\n",
        "    print(\"Loading data from PDFs...\")\n",
        "    documents = SimpleDirectoryReader(pdf_dir).load_data()\n",
        "\n",
        "    print(\"Creating the index...\")\n",
        "    index = GPTVectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    print(\"Saving the index...\")\n",
        "    index.storage_context.persist(storage_dir)\n",
        "\n",
        "    print(\"Index created and saved successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_index(storage_dir):\n",
        "    \"\"\"\n",
        "    Load an existing index from storage.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(storage_dir):\n",
        "        raise FileNotFoundError(f\"Index storage directory not found: {storage_dir}\")\n",
        "\n",
        "    print(\"Loading the index from storage...\")\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
        "    index = load_index_from_storage(storage_context, index_cls=GPTVectorStoreIndex)\n",
        "    print(\"Index loaded successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def query_index(index, query):\n",
        "    \"\"\"\n",
        "    Query the vector index for an answer to the user's question.\n",
        "    \"\"\"\n",
        "    print(f\"Querying the index: {query}\")\n",
        "    query_engine = index.as_query_engine()\n",
        "    response = query_engine.query(query)\n",
        "    return response.response\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pdf_dir = input(\"Enter the PDF directory path (leave blank for default): \") or pdf_directory\n",
        "        storage_dir = input(\"Enter the index storage path (leave blank for default): \") or index_storage_directory\n",
        "\n",
        "        # Try to load the existing index\n",
        "        index = load_index(storage_dir)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"{e}\\nCreating a new index...\")\n",
        "        try:\n",
        "            index = create_index(pdf_dir, storage_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during index creation: {e}\")\n",
        "            sys.exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Chat loop for querying\n",
        "    print(\"Chatbot is ready! Ask questions about the PDFs.\")\n",
        "    try:\n",
        "        while True:\n",
        "            user_input = input(\"Your question (or type 'exit' to quit): \")\n",
        "            if user_input.lower() == \"exit\":\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                answer = query_index(index, user_input)\n",
        "                print(f\"Answer: {answer}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during querying: {e}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nGoodbye!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n23fn34P4HUw"
      },
      "source": [
        " **WITH EVALUATION METRICS and TOP K**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKMy3oOVJ3Ts"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    GPTVectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "import openai\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"API key here\"\n",
        "\n",
        "\n",
        "# Define paths\n",
        "pdf_directory = \"/content/data\"  # Directory containing your PDF file\n",
        "index_storage_directory = \"/content/storage\"  # Directory to store the index\n",
        "\n",
        "# Metrics tracking\n",
        "metrics = defaultdict(int)\n",
        "\n",
        "\n",
        "def create_index():\n",
        "    \"\"\"\n",
        "    Create a new index from PDF files in the specified directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdf_directory) or not os.listdir(pdf_directory):\n",
        "        raise FileNotFoundError(f\"No PDF files found in directory: {pdf_directory}\")\n",
        "\n",
        "    print(\"Loading data from PDF...\")\n",
        "    documents = SimpleDirectoryReader(pdf_directory).load_data()\n",
        "\n",
        "    print(\"Creating the index...\")\n",
        "    index = GPTVectorStoreIndex.from_documents(documents)\n",
        "\n",
        "    print(\"Saving the index...\")\n",
        "    index.storage_context.persist(index_storage_directory)\n",
        "\n",
        "    print(\"Index created and saved successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def load_index():\n",
        "    \"\"\"\n",
        "    Load an existing index from storage.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(index_storage_directory):\n",
        "        raise FileNotFoundError(f\"Index storage directory not found: {index_storage_directory}\")\n",
        "\n",
        "    print(\"Loading the index from storage...\")\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=index_storage_directory)\n",
        "    index = load_index_from_storage(storage_context, index_cls=GPTVectorStoreIndex)\n",
        "    print(\"Index loaded successfully.\")\n",
        "    return index\n",
        "\n",
        "\n",
        "def query_index_with_fallback(index, query, top_k=3, confidence_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Query the vector index with a fallback mechanism.\n",
        "\n",
        "    Parameters:\n",
        "    - index: The GPTVectorStoreIndex object.\n",
        "    - query: The user's query string.\n",
        "    - top_k: The number of top documents to retrieve.\n",
        "    - confidence_threshold: Minimum confidence score to provide a valid response.\n",
        "\n",
        "    Returns:\n",
        "    - A response string based on the query or fallback.\n",
        "    \"\"\"\n",
        "    print(f\"Querying the index: {query}\")\n",
        "    query_engine = index.as_query_engine(top_k=top_k)\n",
        "    response = query_engine.query(query)\n",
        "\n",
        "    # Check if response is valid\n",
        "    if hasattr(response, 'score') and response.score < confidence_threshold:\n",
        "        metrics['fallback_responses'] += 1\n",
        "        return \"I'm sorry, I couldn't find the answer to your question. Please contact our sales team for further assistance.\"\n",
        "\n",
        "    # Fallback if response content is empty\n",
        "    if not response.response or len(response.response.strip()) == 0:\n",
        "        metrics['fallback_responses'] += 1\n",
        "        return \"I'm sorry, I couldn't find the answer to your question. Please contact our sales team for further assistance.\"\n",
        "\n",
        "    return response.response\n",
        "\n",
        "\n",
        "\n",
        "def calculate_metrics():\n",
        "    \"\"\"\n",
        "    Calculate and display evaluation metrics for the chatbot.\n",
        "    \"\"\"\n",
        "    total_queries = metrics['total_queries']\n",
        "    accuracy = (metrics['correct_responses'] / total_queries) * 100 if total_queries else 0\n",
        "    fallback_rate = (metrics['fallback_responses'] / total_queries) * 100 if total_queries else 0\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"Total Queries: {total_queries}\")\n",
        "    print(f\"Correct Responses: {metrics['correct_responses']}\")\n",
        "    print(f\"Fallback Responses: {metrics['fallback_responses']}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Fallback Rate: {fallback_rate:.2f}%\")\n",
        "\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Attempt to load the existing index\n",
        "        index = load_index()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"{e}\\nCreating a new index...\")\n",
        "        index = create_index()\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"Chatbot is ready! Ask questions about the PDF.\")\n",
        "    while True:\n",
        "        user_input = input(\"Your question (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            calculate_metrics()\n",
        "            break\n",
        "\n",
        "        metrics['total_queries'] += 1\n",
        "        try:\n",
        "            # Query the index with fallback\n",
        "            answer = query_index_with_fallback(index, user_input)\n",
        "            print(f\"Answer: {answer}\")\n",
        "\n",
        "            # Log response type (Assume manual evaluation for 'correct' responses)\n",
        "            if \"sales team\" not in answer:\n",
        "                metrics['correct_responses'] += 1\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during querying: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}